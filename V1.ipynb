{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import glob\n",
    "#from text2ipa import get_IPA\n",
    "import re\n",
    "#import spacy\n",
    "\n",
    "def ouvrir_json(chemin):\n",
    "    f = open(chemin, encoding=\"utf-8\")\n",
    "    toto = json.load(f)\n",
    "    f.close()\n",
    "    return toto\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy import spatial\n",
    "\n",
    "def similarity(chaine1, chaine2):#fondé sur une vectorisation + similarité\n",
    "    V = CountVectorizer(ngram_range=(2, 5),analyzer = \"char\")\n",
    "    vect = V.fit_transform([chaine1, chaine2]).toarray()\n",
    "    sim = 1-spatial.distance.cosine(vect[0], vect[1])\n",
    "    return sim\n",
    "\n",
    "def clean_text(text): #nettoyer le texte \n",
    "    data_clean = []\n",
    "    for char in text:\n",
    "        a = re.sub(\"\\n\", \"\", char)\n",
    "        b = re.sub(\"\\\\\\\\\", \"\", a)\n",
    "        c = re.sub(\"\\xa0\", \"\", b)\n",
    "        data_clean.append(c)\n",
    "    return data_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = [] #toutes les questions\n",
    "reponse = []  #toutes les réponses\n",
    "\n",
    "data = ouvrir_json(\"appariements_faq/1_faq_dmc.json\")  #tester le 1er corpus \n",
    "\n",
    "for i in data:\n",
    "    q = data[i][0]\n",
    "    question.append(q)\n",
    "\n",
    "for m in data:\n",
    "    r = data[m][1]\n",
    "    reponse.append(r)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35222345855227477\n",
      "0.26556133458478737\n",
      "0.25279148208080815\n",
      "0.2410114828003278\n",
      "0.2354586404230311\n",
      "0.28889879996138945\n",
      "0.4401313538985685\n",
      "0.28649907248055906\n",
      "0.2582691917986869\n",
      "0.24037693347417322\n",
      "0.23641397187259316\n",
      "0.2745340842894698\n",
      "0.4499533139272027\n",
      "0.3265686480404968\n",
      "0.17988307663828051\n",
      "0.36202997532973424\n",
      "0.42957931611369404\n",
      "0.34940864801858507\n",
      "0.36026030372845175\n",
      "0.19062872325361124\n",
      "0.2405347091353065\n",
      "0.29173063496974194\n",
      "0.3720766784572178\n",
      "0.38616464044086274\n",
      "0.20556242355748822\n"
     ]
    }
   ],
   "source": [
    "clean_ques = clean_text(question)\n",
    "clean_repon = clean_text(reponse)\n",
    "\n",
    "for q in clean_ques:\n",
    "    for r in clean_repon:\n",
    "        print(similarity(q,r))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a maintenant la similarité des questions et des réponses (en utilisant countvectorizer), mais comment visualiser le résultat ? par exemple le taux de reconnaissance 3/5 dans ce cas. Tu as l'idée ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
